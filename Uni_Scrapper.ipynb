{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y5KiQq5kbC-",
        "outputId": "8919a77f-94ea-4939-9407-bff9038e66b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://univ.cc/search?dom=world&key=&start=1\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=51\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=101\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=151\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=201\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=251\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=301\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=351\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=401\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=451\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=501\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=551\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=601\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=651\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=701\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=751\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=801\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=851\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=901\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=951\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1001\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1051\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1101\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1151\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1201\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1251\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1301\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1351\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1401\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1451\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1501\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1551\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1601\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1651\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1701\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1751\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1801\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1851\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1901\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=1951\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2001\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2051\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2101\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2151\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2201\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2251\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2301\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2351\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2401\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2451\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2501\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2551\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2601\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2651\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2701\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2751\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2801\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2851\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2901\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=2951\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3001\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3051\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3101\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3151\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3201\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3251\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3301\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3351\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3401\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3451\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3501\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3551\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3601\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3651\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3701\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3751\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3801\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3851\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3901\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=3951\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4001\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4051\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4101\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4151\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4201\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4251\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4301\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4351\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4401\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4451\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4501\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4551\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4601\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4651\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4701\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4751\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4801\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4851\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4901\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=4951\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5001\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5051\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5101\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5151\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5201\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5251\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5301\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5351\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5401\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5451\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5501\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5551\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5601\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5651\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5701\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5751\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5801\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5851\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5901\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=5951\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6001\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6051\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6101\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6151\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6201\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6251\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6301\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6351\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6401\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6451\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6501\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6551\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6601\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6651\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6701\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6751\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6801\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6851\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6901\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=6951\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7001\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7051\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7101\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7151\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7201\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7251\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7301\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7351\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7401\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7451\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7501\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7551\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7601\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7651\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7701\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7751\n",
            "Scraping: https://univ.cc/search?dom=world&key=&start=7801\n",
            "Scraping complete. All data saved to 'universities.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def scrape_universities_to_csv(output_csv=\"universities.csv\"):\n",
        "    \"\"\"\n",
        "    Scrape the universities from univ.cc (world domain) in steps of 50,\n",
        "    capturing university name, country, and storing the URL as a clickable\n",
        "    Excel-friendly hyperlink in the CSV.\n",
        "    \"\"\"\n",
        "    base_url = \"https://univ.cc/search?dom=world&key=&start=\"\n",
        "\n",
        "    # Open the CSV once in write mode and store the header\n",
        "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        # Write header\n",
        "        writer.writerow([\"University\", \"Country\", \"URL\"])\n",
        "\n",
        "        # We go in steps of 50 results: 1, 51, 101, ... up to 7851\n",
        "        for start in range(1, 7851, 50):\n",
        "            url = f\"{base_url}{start}\"\n",
        "            print(f\"Scraping: {url}\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Find the <ol> that contains the <li> items\n",
        "            ol = soup.find(\"ol\")\n",
        "            if not ol:\n",
        "                # If there's no <ol>, skip\n",
        "                print(f\"No <ol> found on page {url}\")\n",
        "                continue\n",
        "\n",
        "            li_tags = ol.find_all(\"li\")\n",
        "\n",
        "            for li in li_tags:\n",
        "                # Extract the anchor tag\n",
        "                a_tag = li.find(\"a\")\n",
        "                if not a_tag:\n",
        "                    continue\n",
        "\n",
        "                university_name = a_tag.get_text(strip=True)\n",
        "                university_href = a_tag.get(\"href\", \"\").strip()\n",
        "\n",
        "                # Country typically in parentheses at the end of the li text\n",
        "                full_text = li.get_text(strip=True)\n",
        "                # e.g.: \"Some University (SomeCountry)\"\n",
        "                start_paren = full_text.rfind(\"(\")\n",
        "                end_paren = full_text.rfind(\")\")\n",
        "                if start_paren != -1 and end_paren != -1 and end_paren > start_paren:\n",
        "                    country = full_text[start_paren+1 : end_paren].strip()\n",
        "                else:\n",
        "                    country = \"Unknown\"\n",
        "\n",
        "                # Create an Excel-friendly hyperlink formula for the URL\n",
        "                # Format: =HYPERLINK(\"actual_link\", \"display_text\")\n",
        "                # You can choose what text to display. Here, we use the link itself.\n",
        "                hyperlink_formula = f'=HYPERLINK(\"{university_href}\", \"{university_href}\")'\n",
        "\n",
        "                # Write the row\n",
        "                writer.writerow([university_name, country, hyperlink_formula])\n",
        "\n",
        "            # Optionally, sleep a little to avoid hammering the server\n",
        "            # time.sleep(1)\n",
        "\n",
        "    print(f\"Scraping complete. All data saved to '{output_csv}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_universities_to_csv(\"universities.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## US Uni Scraper"
      ],
      "metadata": {
        "id": "cLSzy9TeoYwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "\n",
        "def parse_states(html_text):\n",
        "    \"\"\"\n",
        "    Given the HTML snippet that includes <option> tags for each US state,\n",
        "    parse out a list of tuples: (domain, state_name, count).\n",
        "    Example: (\"edu_al\", \"Alabama\", 39)\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    select_tag = soup.find(\"select\", {\"name\": \"dom\"})\n",
        "    if not select_tag:\n",
        "        return []\n",
        "\n",
        "    options = select_tag.find_all(\"option\")\n",
        "\n",
        "    # We'll store parsed results here\n",
        "    states_data = []\n",
        "\n",
        "    for opt in options:\n",
        "        # Skip the \"* Search in all U.S. States\" or any that don't have 'value'\n",
        "        if not opt.get(\"value\") or opt[\"value\"] == \"world\":\n",
        "            continue\n",
        "\n",
        "        domain = opt[\"value\"].strip()  # e.g., \"edu_al\"\n",
        "        text = opt.get_text(strip=True)  # e.g., \"Alabama (39)\"\n",
        "\n",
        "        # We need to extract the state name and the count (in parentheses)\n",
        "        # text is typically \"Alabama (39)\"\n",
        "        match = re.match(r\"^(.*?)\\s*\\((\\d+)\\)$\", text)\n",
        "        if match:\n",
        "            state_name = match.group(1).strip()\n",
        "            count = int(match.group(2))\n",
        "            states_data.append((domain, state_name, count))\n",
        "        else:\n",
        "            # In case there's no match, skip or handle differently\n",
        "            continue\n",
        "\n",
        "    return states_data\n",
        "\n",
        "def scrape_universities_for_state(domain, state_name, total_count, writer):\n",
        "    \"\"\"\n",
        "    Given a state domain (e.g. 'edu_al'), state name ('Alabama'),\n",
        "    and the total number of universities in that state,\n",
        "    iterate over all pages (in steps of 50), scrape the data, and\n",
        "    write CSV rows using the provided 'writer' (csv.writer).\n",
        "    \"\"\"\n",
        "    base_search_url = \"https://univ.cc/search\"\n",
        "\n",
        "    # For states with more than 50 universities, we do start=1,51,101,... until total_count\n",
        "    for start in range(1, total_count + 1, 50):\n",
        "        params = {\n",
        "            \"dom\": domain,\n",
        "            \"key\": \"\",\n",
        "            \"start\": str(start)\n",
        "        }\n",
        "        # Example: https://univ.cc/search?dom=edu_al&key=&start=1\n",
        "        response = requests.get(base_search_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        ol = soup.find(\"ol\")\n",
        "        if not ol:\n",
        "            # No <ol> found, possibly no results\n",
        "            print(f\"No university listing found for {state_name} at start={start}\")\n",
        "            continue\n",
        "\n",
        "        li_tags = ol.find_all(\"li\")\n",
        "\n",
        "        for li in li_tags:\n",
        "            a_tag = li.find(\"a\")\n",
        "            if not a_tag:\n",
        "                continue\n",
        "\n",
        "            university_name = a_tag.get_text(strip=True)\n",
        "            university_href = a_tag.get(\"href\", \"\").strip()\n",
        "\n",
        "            # Keep the URL as is, but make it clickable in Excel.\n",
        "            # =HYPERLINK(\"actual_url\", \"actual_url\")\n",
        "            hyperlink_formula = f'=HYPERLINK(\"{university_href}\", \"{university_href}\")'\n",
        "\n",
        "            # Write row: University, State, URL\n",
        "            writer.writerow([university_name, state_name, hyperlink_formula])\n",
        "\n",
        "        # Optional delay to avoid sending requests too fast\n",
        "        # time.sleep(1)\n",
        "\n",
        "def main():\n",
        "    # ---- 1) Parse the HTML snippet to get all states  ----\n",
        "    # In practice, you might load this from a file or from another web request,\n",
        "    # but here we store it as a string for demonstration:\n",
        "    html_snippet = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "    </head>\n",
        "    <body>\n",
        "        <main>\n",
        "            <form class=\"form\" method=\"GET\" action=\"/search\">\n",
        "                <div class=\"form-row\">\n",
        "                    <label for=\"dom\">Select a State:</label>\n",
        "                    <select name=\"dom\" size=\"10\">\n",
        "                        <option value=\"world\" selected=\"selected\">* Search in all U.S. States</option>\n",
        "                        <option value=\"edu_al\" >Alabama (39)</option>\n",
        "                        <option value=\"edu_ak\" >Alaska (6)</option>\n",
        "                        <option value=\"edu_az\" >Arizona (21)</option>\n",
        "                        <option value=\"edu_ar\" >Arkansas (23)</option>\n",
        "                        <option value=\"edu_ca\" >California (177)</option>\n",
        "                        <option value=\"edu_co\" >Colorado (29)</option>\n",
        "                        <option value=\"edu_ct\" >Connecticut (29)</option>\n",
        "                        <option value=\"edu_de\" >Delaware (8)</option>\n",
        "                        <option value=\"edu_dc\" >District of Columbia (15)</option>\n",
        "                        <option value=\"edu_fl\" >Florida (68)</option>\n",
        "                        <option value=\"edu_ga\" >Georgia (58)</option>\n",
        "                        <option value=\"edu_hi\" >Hawaii (9)</option>\n",
        "                        <option value=\"edu_id\" >Idaho (8)</option>\n",
        "                        <option value=\"edu_il\" >Illinois (102)</option>\n",
        "                        <option value=\"edu_in\" >Indiana (51)</option>\n",
        "                        <option value=\"edu_ia\" >Iowa (34)</option>\n",
        "                        <option value=\"edu_ks\" >Kansas (25)</option>\n",
        "                        <option value=\"edu_ky\" >Kentucky (27)</option>\n",
        "                        <option value=\"edu_la\" >Louisiana (27)</option>\n",
        "                        <option value=\"edu_me\" >Maine (21)</option>\n",
        "                        <option value=\"edu_md\" >Maryland (34)</option>\n",
        "                        <option value=\"edu_ma\" >Massachusetts (78)</option>\n",
        "                        <option value=\"edu_mi\" >Michigan (61)</option>\n",
        "                        <option value=\"edu_mn\" >Minnesota (46)</option>\n",
        "                        <option value=\"edu_ms\" >Mississippi (20)</option>\n",
        "                        <option value=\"edu_mo\" >Missouri (62)</option>\n",
        "                        <option value=\"edu_mt\" >Montana (10)</option>\n",
        "                        <option value=\"edu_ne\" >Nebraska (23)</option>\n",
        "                        <option value=\"edu_nv\" >Nevada (4)</option>\n",
        "                        <option value=\"edu_nh\" >New Hampshire (17)</option>\n",
        "                        <option value=\"edu_nj\" >New Jersey (32)</option>\n",
        "                        <option value=\"edu_nm\" >New Mexico (11)</option>\n",
        "                        <option value=\"edu_ny\" >New York (159)</option>\n",
        "                        <option value=\"edu_nc\" >North Carolina (57)</option>\n",
        "                        <option value=\"edu_nd\" >North Dakota (16)</option>\n",
        "                        <option value=\"edu_oh\" >Ohio (90)</option>\n",
        "                        <option value=\"edu_ok\" >Oklahoma (28)</option>\n",
        "                        <option value=\"edu_or\" >Oregon (26)</option>\n",
        "                        <option value=\"edu_pa\" >Pennsylvania (117)</option>\n",
        "                        <option value=\"edu_ri\" >Rhode Island (10)</option>\n",
        "                        <option value=\"edu_sc\" >South Carolina (40)</option>\n",
        "                        <option value=\"edu_sd\" >South Dakota (15)</option>\n",
        "                        <option value=\"edu_tn\" >Tennessee (49)</option>\n",
        "                        <option value=\"edu_tx\" >Texas (97)</option>\n",
        "                        <option value=\"edu_ut\" >Utah (11)</option>\n",
        "                        <option value=\"edu_vt\" >Vermont (17)</option>\n",
        "                        <option value=\"edu_va\" >Virginia (53)</option>\n",
        "                        <option value=\"edu_wa\" >Washington (32)</option>\n",
        "                        <option value=\"edu_wv\" >West Virginia (23)</option>\n",
        "                        <option value=\"edu_wi\" >Wisconsin (39)</option>\n",
        "                        <option value=\"edu_wy\" >Wyoming (4)</option>\n",
        "                    </select>\n",
        "                </div>\n",
        "            </form>\n",
        "        </main>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    states_data = parse_states(html_snippet)  # list of (domain, state_name, count)\n",
        "\n",
        "    # ---- 2) Open CSV and write header ----\n",
        "    output_csv = \"us_universities.csv\"\n",
        "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([\"University\", \"State\", \"URL\"])\n",
        "\n",
        "        # ---- 3) For each state, scrape universities ----\n",
        "        for domain, state_name, count in states_data:\n",
        "            print(f\"Scraping {state_name} (total {count} universities)...\")\n",
        "            scrape_universities_for_state(domain, state_name, count, writer)\n",
        "            # time.sleep(1)  # Optional delay if needed\n",
        "\n",
        "    print(f\"Done! Results saved to '{output_csv}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-5-AH6pkerf",
        "outputId": "0d1d9da9-1fed-4f2e-ed3d-7baa3b617ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping Alabama (total 39 universities)...\n",
            "Scraping Alaska (total 6 universities)...\n",
            "Scraping Arizona (total 21 universities)...\n",
            "Scraping Arkansas (total 23 universities)...\n",
            "Scraping California (total 177 universities)...\n",
            "Scraping Colorado (total 29 universities)...\n",
            "Scraping Connecticut (total 29 universities)...\n",
            "Scraping Delaware (total 8 universities)...\n",
            "Scraping District of Columbia (total 15 universities)...\n",
            "Scraping Florida (total 68 universities)...\n",
            "Scraping Georgia (total 58 universities)...\n",
            "Scraping Hawaii (total 9 universities)...\n",
            "Scraping Idaho (total 8 universities)...\n",
            "Scraping Illinois (total 102 universities)...\n",
            "Scraping Indiana (total 51 universities)...\n",
            "Scraping Iowa (total 34 universities)...\n",
            "Scraping Kansas (total 25 universities)...\n",
            "Scraping Kentucky (total 27 universities)...\n",
            "Scraping Louisiana (total 27 universities)...\n",
            "Scraping Maine (total 21 universities)...\n",
            "Scraping Maryland (total 34 universities)...\n",
            "Scraping Massachusetts (total 78 universities)...\n",
            "Scraping Michigan (total 61 universities)...\n",
            "Scraping Minnesota (total 46 universities)...\n",
            "Scraping Mississippi (total 20 universities)...\n",
            "Scraping Missouri (total 62 universities)...\n",
            "Scraping Montana (total 10 universities)...\n",
            "Scraping Nebraska (total 23 universities)...\n",
            "Scraping Nevada (total 4 universities)...\n",
            "Scraping New Hampshire (total 17 universities)...\n",
            "Scraping New Jersey (total 32 universities)...\n",
            "Scraping New Mexico (total 11 universities)...\n",
            "Scraping New York (total 159 universities)...\n",
            "Scraping North Carolina (total 57 universities)...\n",
            "Scraping North Dakota (total 16 universities)...\n",
            "Scraping Ohio (total 90 universities)...\n",
            "Scraping Oklahoma (total 28 universities)...\n",
            "Scraping Oregon (total 26 universities)...\n",
            "Scraping Pennsylvania (total 117 universities)...\n",
            "Scraping Rhode Island (total 10 universities)...\n",
            "Scraping South Carolina (total 40 universities)...\n",
            "Scraping South Dakota (total 15 universities)...\n",
            "Scraping Tennessee (total 49 universities)...\n",
            "Scraping Texas (total 97 universities)...\n",
            "Scraping Utah (total 11 universities)...\n",
            "Scraping Vermont (total 17 universities)...\n",
            "Scraping Virginia (total 53 universities)...\n",
            "Scraping Washington (total 32 universities)...\n",
            "Scraping West Virginia (total 23 universities)...\n",
            "Scraping Wisconsin (total 39 universities)...\n",
            "Scraping Wyoming (total 4 universities)...\n",
            "Done! Results saved to 'us_universities.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MERGE LIST"
      ],
      "metadata": {
        "id": "bqFLVt_PuUpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the first CSV\n",
        "df_universities = pd.read_csv('universities.csv')\n",
        "#   Columns are: University, Country, URL\n",
        "\n",
        "# Read the second CSV\n",
        "df_us_universities = pd.read_csv('us_universities.csv')\n",
        "#   Columns are: University, State, URL\n",
        "\n",
        "# Change 'State' column to 'Country' and set it to 'United States of America'\n",
        "df_us_universities.rename(columns={'State': 'Country'}, inplace=True)\n",
        "df_us_universities['Country'] = 'United States of America'\n",
        "\n",
        "# Concatenate the dataframes\n",
        "df_combined = pd.concat([df_universities, df_us_universities], ignore_index=True)\n",
        "\n",
        "# Save back to the first CSV (or a new file, if you prefer)\n",
        "df_combined.to_csv('universities.csv', index=False)\n"
      ],
      "metadata": {
        "id": "mgB5b8_2ocX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GET RANKINGS"
      ],
      "metadata": {
        "id": "YaN9mzX8wq-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_webometrics_rankings():\n",
        "    # Base URL template; we will append ?page=X\n",
        "    base_url = \"https://www.webometrics.info/en/world?page={}\"\n",
        "\n",
        "    # Open a CSV file to write results\n",
        "    with open(\"world_rankings.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        # Write CSV header\n",
        "        writer.writerow([\"University\", \"Ranking\"])\n",
        "\n",
        "        # The site currently shows 74 pages plus page=0 (so 75 pages total)\n",
        "        for page_num in range(75):\n",
        "            url = base_url.format(page_num)\n",
        "            print(f\"Scraping page: {url}\")\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Warning: Could not retrieve page {page_num}.\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"lxml\")\n",
        "            # The table with the rankings has class \"sticky-enabled\"\n",
        "            table = soup.find(\"table\", {\"class\": \"sticky-enabled\"})\n",
        "            if not table:\n",
        "                # In case no table is found\n",
        "                print(f\"No table found on page {page_num}\")\n",
        "                continue\n",
        "\n",
        "            # The rows of interest are inside the table body\n",
        "            tbody = table.find(\"tbody\")\n",
        "            if not tbody:\n",
        "                continue\n",
        "\n",
        "            rows = tbody.find_all(\"tr\")\n",
        "\n",
        "            for row in rows:\n",
        "                cols = row.find_all(\"td\")\n",
        "                # The first column is ranking, second column is the university name\n",
        "                # Ensure there are enough columns:\n",
        "                if len(cols) >= 2:\n",
        "                    ranking = cols[0].get_text(strip=True)\n",
        "                    university = cols[1].get_text(strip=True)\n",
        "\n",
        "                    # Write to CSV\n",
        "                    writer.writerow([university, ranking])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_webometrics_rankings()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXFxry8kvBPx",
        "outputId": "d2c5ae9c-f058-4e07-9850-90b8cf8306cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page: https://www.webometrics.info/en/world?page=0\n",
            "Scraping page: https://www.webometrics.info/en/world?page=1\n",
            "Scraping page: https://www.webometrics.info/en/world?page=2\n",
            "Scraping page: https://www.webometrics.info/en/world?page=3\n",
            "Scraping page: https://www.webometrics.info/en/world?page=4\n",
            "Scraping page: https://www.webometrics.info/en/world?page=5\n",
            "Scraping page: https://www.webometrics.info/en/world?page=6\n",
            "Scraping page: https://www.webometrics.info/en/world?page=7\n",
            "Scraping page: https://www.webometrics.info/en/world?page=8\n",
            "Scraping page: https://www.webometrics.info/en/world?page=9\n",
            "Scraping page: https://www.webometrics.info/en/world?page=10\n",
            "Scraping page: https://www.webometrics.info/en/world?page=11\n",
            "Scraping page: https://www.webometrics.info/en/world?page=12\n",
            "Scraping page: https://www.webometrics.info/en/world?page=13\n",
            "Scraping page: https://www.webometrics.info/en/world?page=14\n",
            "Scraping page: https://www.webometrics.info/en/world?page=15\n",
            "Scraping page: https://www.webometrics.info/en/world?page=16\n",
            "Scraping page: https://www.webometrics.info/en/world?page=17\n",
            "Scraping page: https://www.webometrics.info/en/world?page=18\n",
            "Scraping page: https://www.webometrics.info/en/world?page=19\n",
            "Scraping page: https://www.webometrics.info/en/world?page=20\n",
            "Scraping page: https://www.webometrics.info/en/world?page=21\n",
            "Scraping page: https://www.webometrics.info/en/world?page=22\n",
            "Scraping page: https://www.webometrics.info/en/world?page=23\n",
            "Scraping page: https://www.webometrics.info/en/world?page=24\n",
            "Scraping page: https://www.webometrics.info/en/world?page=25\n",
            "Scraping page: https://www.webometrics.info/en/world?page=26\n",
            "Scraping page: https://www.webometrics.info/en/world?page=27\n",
            "Scraping page: https://www.webometrics.info/en/world?page=28\n",
            "Scraping page: https://www.webometrics.info/en/world?page=29\n",
            "Scraping page: https://www.webometrics.info/en/world?page=30\n",
            "Scraping page: https://www.webometrics.info/en/world?page=31\n",
            "Scraping page: https://www.webometrics.info/en/world?page=32\n",
            "Scraping page: https://www.webometrics.info/en/world?page=33\n",
            "Scraping page: https://www.webometrics.info/en/world?page=34\n",
            "Scraping page: https://www.webometrics.info/en/world?page=35\n",
            "Scraping page: https://www.webometrics.info/en/world?page=36\n",
            "Scraping page: https://www.webometrics.info/en/world?page=37\n",
            "Scraping page: https://www.webometrics.info/en/world?page=38\n",
            "Scraping page: https://www.webometrics.info/en/world?page=39\n",
            "Scraping page: https://www.webometrics.info/en/world?page=40\n",
            "Scraping page: https://www.webometrics.info/en/world?page=41\n",
            "Scraping page: https://www.webometrics.info/en/world?page=42\n",
            "Scraping page: https://www.webometrics.info/en/world?page=43\n",
            "Scraping page: https://www.webometrics.info/en/world?page=44\n",
            "Scraping page: https://www.webometrics.info/en/world?page=45\n",
            "Scraping page: https://www.webometrics.info/en/world?page=46\n",
            "Scraping page: https://www.webometrics.info/en/world?page=47\n",
            "Scraping page: https://www.webometrics.info/en/world?page=48\n",
            "Scraping page: https://www.webometrics.info/en/world?page=49\n",
            "Scraping page: https://www.webometrics.info/en/world?page=50\n",
            "Scraping page: https://www.webometrics.info/en/world?page=51\n",
            "Scraping page: https://www.webometrics.info/en/world?page=52\n",
            "Scraping page: https://www.webometrics.info/en/world?page=53\n",
            "Scraping page: https://www.webometrics.info/en/world?page=54\n",
            "Scraping page: https://www.webometrics.info/en/world?page=55\n",
            "Scraping page: https://www.webometrics.info/en/world?page=56\n",
            "Scraping page: https://www.webometrics.info/en/world?page=57\n",
            "Scraping page: https://www.webometrics.info/en/world?page=58\n",
            "Scraping page: https://www.webometrics.info/en/world?page=59\n",
            "Scraping page: https://www.webometrics.info/en/world?page=60\n",
            "Scraping page: https://www.webometrics.info/en/world?page=61\n",
            "Scraping page: https://www.webometrics.info/en/world?page=62\n",
            "Scraping page: https://www.webometrics.info/en/world?page=63\n",
            "Scraping page: https://www.webometrics.info/en/world?page=64\n",
            "Scraping page: https://www.webometrics.info/en/world?page=65\n",
            "Scraping page: https://www.webometrics.info/en/world?page=66\n",
            "Scraping page: https://www.webometrics.info/en/world?page=67\n",
            "Scraping page: https://www.webometrics.info/en/world?page=68\n",
            "Scraping page: https://www.webometrics.info/en/world?page=69\n",
            "Scraping page: https://www.webometrics.info/en/world?page=70\n",
            "Scraping page: https://www.webometrics.info/en/world?page=71\n",
            "Scraping page: https://www.webometrics.info/en/world?page=72\n",
            "Scraping page: https://www.webometrics.info/en/world?page=73\n",
            "Scraping page: https://www.webometrics.info/en/world?page=74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WITH RANKS"
      ],
      "metadata": {
        "id": "BTnAaxzqybaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import re\n",
        "\n",
        "# def clean_uni_name(name):\n",
        "#     \"\"\"\n",
        "#     Removes all non-ASCII characters and extra spaces.\n",
        "#     Convert to lowercase to help ensure matching.\n",
        "#     \"\"\"\n",
        "#     if not isinstance(name, str):\n",
        "#         return \"\"\n",
        "#     # Keep only ASCII letters, digits, and basic punctuation/spaces\n",
        "#     # Then strip extra whitespace\n",
        "#     cleaned = re.sub(r'[^\\x00-\\x7F]+','', name).lower().strip()\n",
        "#     return cleaned\n",
        "\n",
        "# # 1. Read the world rankings CSV\n",
        "# df_rankings = pd.read_csv(\"world_rankings.csv\")\n",
        "# # Suppose it has columns: University, Ranking\n",
        "\n",
        "# # 2. Clean the 'University' column in the rankings data\n",
        "# df_rankings[\"clean_name\"] = df_rankings[\"University\"].apply(clean_uni_name)\n",
        "\n",
        "# # 3. Read the universities CSV\n",
        "# df_unis = pd.read_csv(\"universities.csv\")\n",
        "# # Suppose it has columns: University, Country, URL\n",
        "\n",
        "# # 4. Clean the 'University' column in the universities data\n",
        "# df_unis[\"clean_name\"] = df_unis[\"University\"].apply(clean_uni_name)\n",
        "\n",
        "# # 5. Merge the dataframes on cleaned names, using a left join\n",
        "# #    So that all rows from df_unis remain, and ranking merges if matched\n",
        "# df_merged = pd.merge(\n",
        "#     df_unis,\n",
        "#     df_rankings[[\"clean_name\", \"Ranking\"]],\n",
        "#     on=\"clean_name\",\n",
        "#     how=\"left\"\n",
        "# )\n",
        "\n",
        "# # 6. The merged DataFrame now has a 'Ranking' column\n",
        "# #    If no match was found, the 'Ranking' will be NaN\n",
        "# #    (Alternatively, you can fill with an empty string, e.g. fillna(''))\n",
        "# df_merged[\"Ranking\"] = df_merged[\"Ranking\"].fillna('')\n",
        "\n",
        "# # 7. Drop the 'clean_name' column if you no longer need it\n",
        "# df_merged.drop(columns=[\"clean_name\"], inplace=True)\n",
        "\n",
        "# # 8. Save it back to universities.csv (or a new CSV to avoid overwriting)\n",
        "# df_merged.to_csv(\"universities.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "oZYrqfgjwtYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def clean_uni_name(name):\n",
        "    \"\"\"\n",
        "    Removes non-ASCII characters, lowers case, strips whitespace\n",
        "    \"\"\"\n",
        "    if not isinstance(name, str):\n",
        "        return \"\"\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', name).lower().strip()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1) Load & clean df_rankings\n",
        "# ------------------------------------------------------------------------\n",
        "df_rankings = pd.read_csv(\"world_rankings.csv\", encoding='utf-8')\n",
        "# Suppose columns: [University, Ranking]\n",
        "df_rankings[\"clean_name\"] = df_rankings[\"University\"].apply(clean_uni_name)\n",
        "\n",
        "# Reset index so .iloc[...] works with 0..N-1\n",
        "df_rankings.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 2) Load & clean df_unis\n",
        "# ------------------------------------------------------------------------\n",
        "df_unis = pd.read_csv(\"universities.csv\", encoding='utf-8')\n",
        "# Suppose columns: [University, Country, URL] (and possibly 'Ranking' from before).\n",
        "\n",
        "# If the original 'universities.csv' also has a 'Ranking' column,\n",
        "# rename it to avoid collisions:\n",
        "if \"Ranking\" in df_unis.columns:\n",
        "    df_unis.rename(columns={\"Ranking\": \"OriginalRanking\"}, inplace=True)\n",
        "\n",
        "df_unis[\"clean_name\"] = df_unis[\"University\"].apply(clean_uni_name)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 3) Merge on \"clean_name\" for direct matches\n",
        "# ------------------------------------------------------------------------\n",
        "df_merged = pd.merge(\n",
        "    df_unis,\n",
        "    df_rankings[[\"clean_name\", \"University\", \"Ranking\"]],\n",
        "    on=\"clean_name\",\n",
        "    how=\"left\",\n",
        "    suffixes=(\"\", \"_ranks\")  # e.g., 'Ranking_ranks' for the right table\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 4) Create a new \"Matched Uni\" column from direct merges\n",
        "# ------------------------------------------------------------------------\n",
        "df_merged[\"Matched Uni\"] = df_merged[\"University_ranks\"]\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 5) Finalize the single \"Ranking\" column\n",
        "# ------------------------------------------------------------------------\n",
        "# - If \"Ranking_ranks\" exists (the column from df_rankings),\n",
        "#   then adopt it as our main \"Ranking\" column.\n",
        "#   Otherwise, create \"Ranking\" if it doesn't exist yet.\n",
        "if \"Ranking_ranks\" in df_merged.columns:\n",
        "    # Where 'Ranking_ranks' is not NaN, use it\n",
        "    df_merged[\"Ranking\"] = df_merged[\"Ranking_ranks\"]\n",
        "    # Now drop the \"Ranking_ranks\" column\n",
        "    df_merged.drop(columns=[\"Ranking_ranks\"], inplace=True)\n",
        "\n",
        "# If \"Ranking\" did not exist at all (e.g. if we never had a collisions), ensure it does:\n",
        "if \"Ranking\" not in df_merged.columns:\n",
        "    df_merged[\"Ranking\"] = np.nan\n",
        "\n",
        "# Convert \"Ranking\" & \"Matched Uni\" to string so we can assign \"\"\n",
        "df_merged[\"Ranking\"] = df_merged[\"Ranking\"].astype(str)\n",
        "df_merged[\"Matched Uni\"] = df_merged[\"Matched Uni\"].astype(str)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 6) Embeddings fallback for rows where \"Ranking\" is missing\n",
        "# ------------------------------------------------------------------------\n",
        "missing_mask = (\n",
        "    df_merged[\"Ranking\"].eq(\"nan\") |\n",
        "    df_merged[\"Ranking\"].eq(\"None\") |\n",
        "    df_merged[\"Ranking\"].eq(\"\")\n",
        ")\n",
        "if missing_mask.any():\n",
        "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    # Subset those rows\n",
        "    df_missing = df_merged.loc[missing_mask].copy()\n",
        "\n",
        "    # Encode their clean_names\n",
        "    missing_emb = model.encode(df_missing[\"clean_name\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "    # Encode all ranking unis\n",
        "    rank_emb = model.encode(df_rankings[\"clean_name\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "    # Fallback search\n",
        "    for i, row_idx in enumerate(df_missing.index):\n",
        "        this_emb = missing_emb[i]\n",
        "\n",
        "        # Cosine similarities\n",
        "        cos_scores = util.cos_sim(this_emb, rank_emb)[0]  # shape: [N]\n",
        "        best_idx = cos_scores.argmax().item()\n",
        "        best_score = float(cos_scores[best_idx].item())\n",
        "\n",
        "        # Lower threshold to 0.70\n",
        "        if best_score >= 0.70:\n",
        "            matched_name = df_rankings.iloc[best_idx][\"University\"]\n",
        "            matched_rank = df_rankings.iloc[best_idx][\"Ranking\"]\n",
        "            df_merged.at[row_idx, \"Matched Uni\"] = matched_name\n",
        "            df_merged.at[row_idx, \"Ranking\"] = str(matched_rank)\n",
        "        else:\n",
        "            df_merged.at[row_idx, \"Matched Uni\"] = \"\"\n",
        "            df_merged.at[row_idx, \"Ranking\"] = \"\"\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 7) Tidy up leftover columns & save\n",
        "# ------------------------------------------------------------------------\n",
        "# \"University_ranks\" is from the merge. Remove it if still present.\n",
        "if \"University_ranks\" in df_merged.columns:\n",
        "    df_merged.drop(columns=[\"University_ranks\"], inplace=True)\n",
        "\n",
        "# Also remove the \"clean_name\" helper\n",
        "if \"clean_name\" in df_merged.columns:\n",
        "    df_merged.drop(columns=[\"clean_name\"], inplace=True)\n",
        "\n",
        "# Save final\n",
        "df_merged.to_csv(\"universities_4.csv\", index=False)\n",
        "\n",
        "print(\"Done! 'universities_4.csv' now has a single 'Ranking' column and a 'Matched Uni' column, with no leftover '_ranks' columns.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcIVg6su008g",
        "outputId": "432c673f-865d-4874-afc0-24cb752efce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! 'universities_4.csv' now has a single 'Ranking' column and a 'Matched Uni' column, with no leftover '_ranks' columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_merged.drop(columns=[\"clean_name\", \"University_ranks\"], inplace=True, errors=\"ignore\")\n",
        "df_merged.to_csv(\"universities_2.csv\", index=False)"
      ],
      "metadata": {
        "id": "HJbZilFD_UxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "K6GeNUp6yeen",
        "outputId": "56719d92-7e8e-4865-e20e-044b725ca08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            University  Country  \\\n",
              "0  1 December University of Alba Iulia  Romania   \n",
              "1      2nd Military Medical University    China   \n",
              "2      3rd Military Medical University    China   \n",
              "3      4th Military Medical University    China   \n",
              "4                          AAB College   Kosovo   \n",
              "5             Aalborg Business College  Denmark   \n",
              "6                   Aalborg University  Denmark   \n",
              "7                     Aalto University  Finland   \n",
              "8        Aarhus School of Architecture  Denmark   \n",
              "9            Aarhus School of Business  Denmark   \n",
              "\n",
              "                                                 URL Ranking  Ranking_ranks  \\\n",
              "0  =HYPERLINK(\"http://www.uab.ro/\", \"http://www.u...                    NaN   \n",
              "1  =HYPERLINK(\"http://www.smmu.edu.cn/\", \"http://...    1411            NaN   \n",
              "2  =HYPERLINK(\"http://www.tmmu.edu.cn/\", \"http://...    1915            NaN   \n",
              "3  =HYPERLINK(\"https://www.fmmu.edu.cn/\", \"https:...    1330            NaN   \n",
              "4  =HYPERLINK(\"https://aab-edu.net/\", \"https://aa...                    NaN   \n",
              "5  =HYPERLINK(\"http://www.ah.dk/\", \"http://www.ah...                    NaN   \n",
              "6  =HYPERLINK(\"http://www.aau.dk/\", \"http://www.a...                    NaN   \n",
              "7  =HYPERLINK(\"http://www.aalto.fi/\", \"http://www...                    NaN   \n",
              "8  =HYPERLINK(\"http://www.a-aarhus.dk/\", \"http://...                    NaN   \n",
              "9  =HYPERLINK(\"http://bss.au.dk/\", \"http://bss.au...                    NaN   \n",
              "\n",
              "                                       Matched Uni  \n",
              "0                                                   \n",
              "1  The Second Military Medical University / 第二军医大学  \n",
              "2       Third Military Medical University / 第三军医大学  \n",
              "3  The Fourth Military Medical University / 第四军医大学  \n",
              "4                                                   \n",
              "5                                                   \n",
              "6                                                   \n",
              "7                                                   \n",
              "8                                                   \n",
              "9                                                   "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-258b8ee9-7421-4692-89d5-67a733dc959f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>University</th>\n",
              "      <th>Country</th>\n",
              "      <th>URL</th>\n",
              "      <th>Ranking</th>\n",
              "      <th>Ranking_ranks</th>\n",
              "      <th>Matched Uni</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 December University of Alba Iulia</td>\n",
              "      <td>Romania</td>\n",
              "      <td>=HYPERLINK(\"http://www.uab.ro/\", \"http://www.u...</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2nd Military Medical University</td>\n",
              "      <td>China</td>\n",
              "      <td>=HYPERLINK(\"http://www.smmu.edu.cn/\", \"http://...</td>\n",
              "      <td>1411</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Second Military Medical University / 第二军医大学</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3rd Military Medical University</td>\n",
              "      <td>China</td>\n",
              "      <td>=HYPERLINK(\"http://www.tmmu.edu.cn/\", \"http://...</td>\n",
              "      <td>1915</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Third Military Medical University / 第三军医大学</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4th Military Medical University</td>\n",
              "      <td>China</td>\n",
              "      <td>=HYPERLINK(\"https://www.fmmu.edu.cn/\", \"https:...</td>\n",
              "      <td>1330</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Fourth Military Medical University / 第四军医大学</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAB College</td>\n",
              "      <td>Kosovo</td>\n",
              "      <td>=HYPERLINK(\"https://aab-edu.net/\", \"https://aa...</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Aalborg Business College</td>\n",
              "      <td>Denmark</td>\n",
              "      <td>=HYPERLINK(\"http://www.ah.dk/\", \"http://www.ah...</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Aalborg University</td>\n",
              "      <td>Denmark</td>\n",
              "      <td>=HYPERLINK(\"http://www.aau.dk/\", \"http://www.a...</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Aalto University</td>\n",
              "      <td>Finland</td>\n",
              "      <td>=HYPERLINK(\"http://www.aalto.fi/\", \"http://www...</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Aarhus School of Architecture</td>\n",
              "      <td>Denmark</td>\n",
              "      <td>=HYPERLINK(\"http://www.a-aarhus.dk/\", \"http://...</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Aarhus School of Business</td>\n",
              "      <td>Denmark</td>\n",
              "      <td>=HYPERLINK(\"http://bss.au.dk/\", \"http://bss.au...</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-258b8ee9-7421-4692-89d5-67a733dc959f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-258b8ee9-7421-4692-89d5-67a733dc959f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-258b8ee9-7421-4692-89d5-67a733dc959f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fb32435c-99d6-4e8b-bbfe-ae94f1e6487c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb32435c-99d6-4e8b-bbfe-ae94f1e6487c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fb32435c-99d6-4e8b-bbfe-ae94f1e6487c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_merged",
              "summary": "{\n  \"name\": \"df_merged\",\n  \"rows\": 9911,\n  \"fields\": [\n    {\n      \"column\": \"University\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9788,\n        \"samples\": [\n          \"University of Texas at Dallas\",\n          \"University of Medicine and Pharmacology of Oradea\",\n          \"Sri Sant Gajanan Maharaj College of Engineering\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 209,\n        \"samples\": [\n          \"Philippines\",\n          \"Equatorial Guinea\",\n          \"New Zealand\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9812,\n        \"samples\": [\n          \"=HYPERLINK(\\\"http://www.lshtm.ac.uk/\\\", \\\"http://www.lshtm.ac.uk/\\\")\",\n          \"=HYPERLINK(\\\"https://psbaqc.edu.ph/\\\", \\\"https://psbaqc.edu.ph/\\\")\",\n          \"=HYPERLINK(\\\"http://www.ful.ac.be/\\\", \\\"http://www.ful.ac.be/\\\")\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ranking\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3498,\n        \"samples\": [\n          \"370\",\n          \"1274\",\n          \"5676\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ranking_ranks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2160.3486955486896,\n        \"min\": 1.0,\n        \"max\": 7497.0,\n        \"num_unique_values\": 1785,\n        \"samples\": [\n          2411.0,\n          464.0,\n          7453.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Matched Uni\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4198,\n        \"samples\": [\n          \"Universit\\u00e4t fur Angewandte Kunst Wien\",\n          \"Tver State University / \\u0422\\u0432\\u0435\\u0440\\u0441\\u043a\\u043e\\u0439 \\u0433\\u043e\\u0441\\u0443\\u0434\\u0430\\u0440\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0439 \\u0443\\u043d\\u0438\\u0432\\u0435\\u0440\\u0441\\u0438\\u0442\\u0435\\u0442\",\n          \"Wartburg College\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rk8t9Wwl1kdn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}